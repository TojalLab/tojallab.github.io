[{"data":1,"prerenderedAt":460},["Reactive",2],{"content-query-PdDh2wRN2L":3,"content-query-vCYVs9Pm92":54,"content-query-5JHBW92uwA":97,"content-query-GJW6JsTxeE":395,"content-query-7ELIWx6IJ9":425},[4],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":9,"description":10,"authors":11,"category":15,"image":19,"bib":20,"links":21,"body":28,"_type":49,"_id":50,"_source":51,"_file":52,"_extension":53},"/publications/mil-cancers-paper","publications",false,"","Multiple Instance Learning (MIL) to predict DRD from histopathological images","DNA repair deficiency (DRD) is an important driver of carcinogenesis and an efficient target for anti-tumor therapies to improve patient survival",[12,13,14],"Renan Valieris","Outro","Israel Tojal da Silva",[16,17,18],"Machine learning","Imaging","Multiple Instance Learning (MIL)","/img/cancers-12-03687-g002.png","@article{valieris2020deep, title={Deep learning predicts underlying features on pathology images with therapeutic relevance for breast and gastric cancer}, author={Valieris, Renan and Amaro, Lucas and Osorio, Cynthia Aparecida Bueno de Toledo and Bueno, Adriana Passos and Rosales Mitrowsky, Rafael Andres and Carraro, Dirce Maria and Nunes, Diana Noronha and Dias-Neto, Emmanuel and Silva, Israel Tojal da}, journal={Cancers}, volume={12}, number={12}, pages={3687}, year={2020}, publisher={MDPI} }",[22,25],{"title":23,"url":24},"Paper","https://www.mdpi.com/2072-6694/12/12/3687",{"title":26,"url":27},"Github repository","https://github.com/rvalieris/deepHE",{"type":29,"children":30,"toc":46},"root",[31,40],{"type":32,"tag":33,"props":34,"children":36},"element","h1",{"id":35},"deep-learning-predicts-underlying-features-on-pathology-images-with-therapeutic-relevance-for-breast-and-gastric-cancer",[37],{"type":38,"value":39},"text","Deep Learning Predicts Underlying Features on Pathology Images with Therapeutic Relevance for Breast and Gastric Cancer",{"type":32,"tag":41,"props":42,"children":43},"p",{},[44],{"type":38,"value":45},"DNA repair deficiency (DRD) is an important driver of carcinogenesis and an efficient target for anti-tumor therapies to improve patient survival. Thus, detection of DRD in tumors is paramount. Currently, determination of DRD in tumors is dependent on wet-lab assays. Here we describe an efficient machine learning algorithm which can predict DRD from histopathological images. The utility of this algorithm is demonstrated with data obtained from 1445 cancer patients. Our method performs rather well when trained on breast cancer specimens with homologous recombination deficiency (HRD), AUC (area under curve) = 0.80. Results for an independent breast cancer cohort achieved an AUC = 0.70. The utility of our method was further shown by considering the detection of mismatch repair deficiency (MMRD) in gastric cancer, yielding an AUC = 0.81. Our results demonstrate the capacity of our learning-base system as a low-cost tool for DRD detection.",{"title":8,"searchDepth":47,"depth":47,"links":48},2,[],"markdown","content:publications:mil-cancers-paper.md","content","publications/mil-cancers-paper.md","md",[55],{"_path":56,"_dir":57,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":58,"description":59,"category":60,"body":62,"_type":49,"_id":95,"_source":51,"_file":96,"_extension":53},"/news/website-live","news","New website is now live!","We are excited to announce that our website is now live! ðŸŽ‰",[61],"News",{"type":29,"children":63,"toc":93},[64,70,74,79],{"type":32,"tag":33,"props":65,"children":67},{"id":66},"tojal-lab",[68],{"type":38,"value":69},"Tojal Lab",{"type":32,"tag":41,"props":71,"children":72},{},[73],{"type":38,"value":59},{"type":32,"tag":41,"props":75,"children":76},{},[77],{"type":38,"value":78},"On our website, you can learn more about our laboratory, our research areas, and our collaborators. You can also\nbrowse our publications, read abstracts and full-text articles, and download citations. You can also meet our team\nmembers, and contact them for any inquiries or feedback. We have also integrated a news section where we will share\nannouncements, events, awards, and media coverage related to our laboratory.",{"type":32,"tag":41,"props":80,"children":81},{},[82,84,91],{"type":38,"value":83},"Thank you for your interest in our laboratory. Checkout our ",{"type":32,"tag":85,"props":86,"children":88},"a",{"href":87},"/joinus",[89],{"type":38,"value":90},"join us",{"type":38,"value":92}," page.",{"title":8,"searchDepth":47,"depth":47,"links":94},[],"content:news:website-live.md","news/website-live.md",[98,169],{"_path":99,"_dir":100,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":101,"description":102,"category":103,"bib":106,"links":107,"body":112,"_type":49,"_id":167,"_source":51,"_file":168,"_extension":53},"/software/signer","software","Signer","Signer is an Open Source library written in R for signature something",[104,105],"Bioinformatics","Signature","@article{rosales2017signer,  title={signeR: an empirical Bayesian approach to mutational signature discovery},  author={Rosales, Rafael A and Drummond, Rodrigo D and Valieris, Renan and Dias-Neto, Emmanuel and Da Silva, Israel T},  journal={Bioinformatics},  volume={33},  number={1},  pages={8--16},  year={2017},  publisher={Oxford University Press}}",[108,110],{"title":23,"url":109},"https://academic.oup.com/bioinformatics/article-abstract/33/1/8/2525683",{"title":26,"url":111},"https://github.com/TojalLab/signeR",{"type":29,"children":113,"toc":162},[114,119,126,131,137,142,148],{"type":32,"tag":33,"props":115,"children":117},{"id":116},"signer",[118],{"type":38,"value":101},{"type":32,"tag":120,"props":121,"children":123},"h2",{"id":122},"motivation",[124],{"type":38,"value":125},"Motivation",{"type":32,"tag":41,"props":127,"children":128},{},[129],{"type":38,"value":130},"Mutational signatures can be used to understand cancer origins and provide a unique opportunity to group tumor types that share the same origins and result from similar processes. These signatures have been identified from high throughput sequencing data generated from cancer genomes by using non-negative matrix factorisation (NMF) techniques. Current methods based on optimization techniques are strongly sensitive to initial conditions due to high dimensionality and nonconvexity of the NMF paradigm. In this context, an important question consists in the determination of the actual number of signatures that best represent the data. The extraction of mutational signatures from high-throughput data still remains a daunting task.",{"type":32,"tag":120,"props":132,"children":134},{"id":133},"results",[135],{"type":38,"value":136},"Results",{"type":32,"tag":41,"props":138,"children":139},{},[140],{"type":38,"value":141},"Here we present a new method for the statistical estimation of mutational signatures based on an empirical Bayesian treatment of the NMF model. While requiring minimal intervention from the user, our method addresses the determination of the number of signatures directly as a model selection problem. In addition, we introduce two new concepts of significant clinical relevance for evaluating the mutational profile. The advantages brought by our approach are shown by the analysis of real and synthetic data. The later is used to compare our approach against two alternative methods mostly used in the literature and with the same NMF parametrization as the one considered here. Our approach is robust to initial conditions and more accurate than competing alternatives. It also estimates the correct number of signatures even when other methods fail. Results on real data agree well with current knowledge.",{"type":32,"tag":120,"props":143,"children":145},{"id":144},"availability-and-implementation",[146],{"type":38,"value":147},"Availability and Implementation",{"type":32,"tag":41,"props":149,"children":150},{},[151,153,160],{"type":38,"value":152},"signeR is implemented in R and C++, and is available as a R package at ",{"type":32,"tag":85,"props":154,"children":158},{"href":155,"rel":156},"http://bioconductor.org/packages/signeR",[157],"nofollow",[159],{"type":38,"value":155},{"type":38,"value":161},".",{"title":8,"searchDepth":47,"depth":47,"links":163},[164,165,166],{"id":122,"depth":47,"text":125},{"id":133,"depth":47,"text":136},{"id":144,"depth":47,"text":147},"content:software:Signer.md","software/Signer.md",{"_path":170,"_dir":100,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":171,"description":172,"image":173,"authors":174,"category":176,"links":179,"body":185,"_type":49,"_id":393,"_source":51,"_file":394,"_extension":53},"/software/wsi2ml","WSI2ML - Digital Pathology Image annotator tool","The WSI2ML is Whole Slide Image annotation software focusing on generating high quality image datasets for Machine Learning.","/img/wsi2ml.jpg",[175,14],"Luan V. C. Martins",[17,177,178,16],"Annotations","Dataset",[180,183],{"title":181,"url":182},"Software documentation","https://luanvcmartins.github.io/WSI2ML/",{"title":26,"url":184},"https://github.com/luanvcmartins/WSI2ML",{"type":29,"children":186,"toc":382},[187,193,198,203,208,214,221,226,252,259,264,277,282,288,293,299,312,330,336,347,353,358,364,369],{"type":32,"tag":33,"props":188,"children":190},{"id":189},"wsi2ml",[191],{"type":38,"value":192},"WSI2ML",{"type":32,"tag":41,"props":194,"children":195},{},[196],{"type":38,"value":197},"WSI2ML is a straightforward tool for annotating Whole Slide Images (WSIs) for machine learning applications. WSIs are\nhigh-resolution digital images of entire tissue sections, often used in medical research and diagnosis. Annotating these\nimages can be a time-consuming and challenging task, but it is critical for training accurate machine learning models.",{"type":32,"tag":41,"props":199,"children":200},{},[201],{"type":38,"value":202},"The goal of WSI2ML is to streamline the annotation process, making it easy researchers and clinicians to label WSIs, and\nfor machine learning specialists to export and use the data for training new machine learning applications. The software\nintroduces a task-based approach to annotating data, which simplifies the process of training new annotators, tracking\ntheir progress and delegating new tasks. These annotations can then be used to train machine learning models to\naccurately diagnose diseases or predict patient outcomes.",{"type":32,"tag":41,"props":204,"children":205},{},[206],{"type":38,"value":207},"Developed as web-application, the software can be installed on a server and made accessible to users in your\norganization.",{"type":32,"tag":120,"props":209,"children":211},{"id":210},"ml-based-wsi-workflow",[212],{"type":38,"value":213},"ML-based WSI workflow",{"type":32,"tag":41,"props":215,"children":216},{},[217],{"type":32,"tag":218,"props":219,"children":220},"img",{"alt":218,"src":173},[],{"type":32,"tag":41,"props":222,"children":223},{},[224],{"type":38,"value":225},"Our goal with the software was to streamline the annotation, revision and ML-model interpretability when dealing with\nmulti-giga images. In particular, we need to fill the two following gaps we found lacking in currently available\nannotation software:",{"type":32,"tag":227,"props":228,"children":229},"ul",{},[230,242],{"type":32,"tag":231,"props":232,"children":233},"li",{},[234,240],{"type":32,"tag":235,"props":236,"children":237},"strong",{},[238],{"type":38,"value":239},"Simplified annotation process",{"type":38,"value":241},": the first step in generating a new image dataset is to have domain specialists\nannotate the image regions, this is often a daunting task if the software is too complicated use. To efficiently get\nnew domain-specialists on board, the tool must be as simple as possible to use to make the experience of training and\nadapting willing domain specialists easier.",{"type":32,"tag":231,"props":243,"children":244},{},[245,250],{"type":32,"tag":235,"props":246,"children":247},{},[248],{"type":38,"value":249},"Interpreting the model",{"type":38,"value":251},": the multigiga nature of those images makes it hard to get human feedback and understand\nhow the model behaves. In fact, domain specialists who are key in this aspect of research will often require full\naccess to the images in order to properly provide feedback on how the models work.",{"type":32,"tag":253,"props":254,"children":256},"h3",{"id":255},"annotate-image",[257],{"type":38,"value":258},"Annotate image",{"type":32,"tag":41,"props":260,"children":261},{},[262],{"type":38,"value":263},"The figure of the domain specialist is key in this research field, as they are needed in order to provide a wide range\nof input samples to train our deep learning models. The challenge is to recruit and train the specialist to be able to\nprovide the required domain-knowledge to train our models. It's of the upmost importance to make their experience of\nusing the software as smooth as possible, so they can focus on their task: provide carefully made annotations.",{"type":32,"tag":41,"props":265,"children":266},{},[267,269,275],{"type":38,"value":268},"The domain specialist experience using the software is simple: upon login, they can see their status page, which\nshows their progress and what images are remaining to be annotated. To start annotating, the user can simply press the\n",{"type":32,"tag":270,"props":271,"children":272},"em",{},[273],{"type":38,"value":274},"continue button",{"type":38,"value":276}," or choose a specific image that was delegated to them in the list.",{"type":32,"tag":41,"props":278,"children":279},{},[280],{"type":38,"value":281},"In particular, our task-based approach allows not only this simplified experience, but also allow more granular\ncontrol over who is expected to annotate what images.  Moreover, by using tasks, we add accountability into the mix: users\ncan track and quantify their progress, taking ownership of their work and participation on the success of the research\nendeavour.",{"type":32,"tag":253,"props":283,"children":285},{"id":284},"revising-annotations",[286],{"type":38,"value":287},"Revising annotations",{"type":32,"tag":41,"props":289,"children":290},{},[291],{"type":38,"value":292},"Once a task is completed, revision tasks can be made in the system so other pathologists can provide feedback on the\nannotations provided. This feedback is taken into account when exporting annotations, where the machine learning\nspecialist can choose to ignore annotations with low agree-ability.",{"type":32,"tag":253,"props":294,"children":296},{"id":295},"exporting-annotations",[297],{"type":38,"value":298},"Exporting annotations",{"type":32,"tag":41,"props":300,"children":301},{},[302,304,310],{"type":38,"value":303},"Annotations made through the software can be exported in any stage of the process as long as there are tasks\nmarked as completed. The annotations are exported in the ",{"type":32,"tag":305,"props":306,"children":307},"code-inline",{},[308],{"type":38,"value":309},"geojson",{"type":38,"value":311}," format, which provides the geometry and metadata\ninformation of the annotation for each annotated slide.",{"type":32,"tag":41,"props":313,"children":314},{},[315,317,322,324,329],{"type":38,"value":316},"ML specialists can customize the annotations exported, download the dynamically generated ",{"type":32,"tag":305,"props":318,"children":319},{},[320],{"type":38,"value":321},"zip",{"type":38,"value":323}," file, and use its\ndata to train their model as needed. This approach was chosen to allow any type of machine learning algorithm to be\nused. However, we also have provided a public python package to work with ",{"type":32,"tag":305,"props":325,"children":326},{},[327],{"type":38,"value":328},"image tiles",{"type":38,"value":161},{"type":32,"tag":253,"props":331,"children":333},{"id":332},"visualizing-and-interpreting-ml-model",[334],{"type":38,"value":335},"Visualizing and interpreting ML-model",{"type":32,"tag":41,"props":337,"children":338},{},[339,341,345],{"type":38,"value":340},"The publicly available annotation handling library also has utility functions to generate ",{"type":32,"tag":305,"props":342,"children":343},{},[344],{"type":38,"value":309},{"type":38,"value":346}," files that can\nbe uploaded into back the software to visualize the model as annotations. This is particularly useful when\nattempting to understand the model's inner workings and shortcomings.",{"type":32,"tag":253,"props":348,"children":350},{"id":349},"improving-annotations",[351],{"type":38,"value":352},"Improving annotations",{"type":32,"tag":41,"props":354,"children":355},{},[356],{"type":38,"value":357},"Lastly, once the ML model's results are available, domain specialists can continue to work on their annotations in\norder to improve model performance. This step was crucial in our image-related research, as it allowed us to\ndynamically understand the challenges, propose changes and guide our research process.",{"type":32,"tag":120,"props":359,"children":361},{"id":360},"install",[362],{"type":38,"value":363},"Install",{"type":32,"tag":41,"props":365,"children":366},{},[367],{"type":38,"value":368},"Follow the instructions available at the official documentation website:",{"type":32,"tag":227,"props":370,"children":371},{},[372],{"type":32,"tag":231,"props":373,"children":374},{},[375,381],{"type":32,"tag":85,"props":376,"children":378},{"href":182,"rel":377},[157],[379],{"type":38,"value":380},"Official documentation",{"type":38,"value":161},{"title":8,"searchDepth":47,"depth":47,"links":383},[384,392],{"id":210,"depth":47,"text":213,"children":385},[386,388,389,390,391],{"id":255,"depth":387,"text":258},3,{"id":284,"depth":387,"text":287},{"id":295,"depth":387,"text":298},{"id":332,"depth":387,"text":335},{"id":349,"depth":387,"text":352},{"id":360,"depth":47,"text":363},"content:software:WSI2ML.md","software/WSI2ML.md",{"_path":396,"_dir":8,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":14,"description":397,"body":398,"_type":49,"_id":423,"_source":51,"_file":424,"_extension":53},"/israel","Associate Researcher at Rockefeller University (2014-2015) and laboratory leader. Since 2015 he has been a Scientist at FundaÃ§Ã£o AntÃ´nio Prudente-A.C.Camargo Cancer Center, where he leads the Bioinformatics Laboratory. He has experience in the area of Computer Science, with an emphasis on Bioinformatics, working mainly on the following topics: genomics and transcriptomics.",{"type":29,"children":399,"toc":420},[400,405,409,415],{"type":32,"tag":33,"props":401,"children":403},{"id":402},"israel-tojal-da-silva",[404],{"type":38,"value":14},{"type":32,"tag":41,"props":406,"children":407},{},[408],{"type":38,"value":397},{"type":32,"tag":120,"props":410,"children":412},{"id":411},"background",[413],{"type":38,"value":414},"Background",{"type":32,"tag":41,"props":416,"children":417},{},[418],{"type":38,"value":419},"Degree in Computer Science from Universidade Paulista (2001), Specialization in Bioinformatics from the National Scientific Computing Laboratory (2003), Doctorate in Sciences from the University of SÃ£o Paulo (2009) and Post Doctorate from Rockefeller University in the USA (2014).",{"title":8,"searchDepth":47,"depth":47,"links":421},[422],{"id":411,"depth":47,"text":414},"content:israel.md","israel.md",{"_path":426,"_dir":8,"_draft":7,"_partial":7,"_locale":8,"_empty":7,"title":427,"description":428,"body":429,"_type":49,"_id":458,"_source":51,"_file":459,"_extension":53},"/lab","About Tojal Lab","We are a SÃ£o Paulo-based research group that is interested in bioinformatics, genomics and imaging, focusing on its\nintersection with machine learning and artificial intelligence. We aim to develop novel methods and tools to analyze,\nintegrate, and interpret large-scale biological data, and to apply them to address real-world problems in biomedicine\nand biotechnology.",{"type":29,"children":430,"toc":456},[431,436,440,445],{"type":32,"tag":33,"props":432,"children":434},{"id":433},"about-tojal-lab",[435],{"type":38,"value":427},{"type":32,"tag":41,"props":437,"children":438},{},[439],{"type":38,"value":428},{"type":32,"tag":41,"props":441,"children":442},{},[443],{"type":38,"value":444},"Our laboratory is composed of talented and motivated researchers from diverse backgrounds and disciplines, such as\ncomputer science, mathematics, statistics, biology, and medicine. Our goal is to work  collaboratively and\ncreatively to advance the\nfrontiers of knowledge and innovation in our field.",{"type":32,"tag":41,"props":446,"children":447},{},[448,450,454],{"type":38,"value":449},"We are always looking for new members who share our passion and vision. Check out the ",{"type":32,"tag":85,"props":451,"children":452},{"href":87},[453],{"type":38,"value":90},{"type":38,"value":455}," page for more\ninformation on the available positions, requirements, and application\nprocess.",{"title":8,"searchDepth":47,"depth":47,"links":457},[],"content:lab.md","lab.md",1696121329210]